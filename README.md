# VERSE: Virtual Environment for Rendering of Speech Emissions

This is a project focusing on human voice perception with different purposes. 
Among the others, the main goal is to study the advantages of an array of microphones versus binaural audio signals, in the context of real embedded devices and including machine learning algorithms (in particular neural networks) for signal processing, with a particular focus on human voices.

**VERSE** contains a semi-synthetic dataset of voice recordings and real environment characterization measurements.
It includes a complete software framework able to generate synthetic audio data from measures on the real setup, keeping the result acoustically as close as possible to the equivalent "direct recording".

**VERSE** is released as an open source repository serving both functions: providing a flexible dataset and providing a tool to customize the dataset itself based on the specific subject under study.

Even if VERSE originally was designed to study the performance of an array of microphones placed on a pair of glasses, and to compare results with binaural earing, the flexibility of the framework allows the user to generate sequences using any microphone geometry. This extends the usability of VERSE tool chain to use cases different from multimedia glasses, like robotic heads for industrial applications, or helmets for safety awareness, allowing the optimization of microphone placement to obtain the best results with the provided subject.

**VERSE** is based on the abstraction of main components for an audio scene: voice sound sources from human **speakers**, one **listener** (the "head" of the device under test) and reverberation generated by the environment itself, meaning the **room** hosting speakers and listener. Specific to the definition of a scene is the concept of **motion: the scene defines how sound sources are placed around the listener and how they move in space**.
