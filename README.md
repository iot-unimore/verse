# VERSE: Virtual Environment for Rendering of Speech Emissions

This is a project focusing on human voice perception with different purposes. 
Among the others, the main goal is to study the advantages of an array of microphones versus binaural audio signals, in the context of real embedded devices and including machine learning algorithms (in particular neural networks) for signal processing, with a particular focus on human voices.

**VERSE** contains a semi-synthetic dataset of voice recordings and real environment characterization measurements.
It includes a complete software framework able to generate synthetic audio data from measures on the real setup, keeping the result acoustically as close as possible to the equivalent "direct recording".

**VERSE** is released as an open source repository serving both functions: providing a flexible dataset and providing a tool to customize the dataset itself based on the specific subject under study.

<img src="/docs/pics/verse_arch_schema.png" align="left" width="200px"/>

Even if VERSE originally was designed to study the performance of an array of microphones placed on a pair of glasses, and to compare results with binaural earing, the flexibility of the framework allows the user to generate sequences using any microphone geometry. This extends the usability of VERSE tool chain to use cases different from multimedia glasses, like robotic heads for industrial applications, or helmets for safety awareness, allowing the optimization of microphone placement to obtain the best results with the provided subject.

**VERSE** is based on the abstraction of main components for an audio scene: voice sound sources from human **speakers**, one **listener** (the "head" of the device under test) and reverberation generated by the environment itself, meaning the **room** hosting speakers and listener. Specific to the definition of a scene is the concept of **motion: the scene defines how sound sources are placed around the listener and how they move in space**.


<br clear="left"/>

# The Dataset
This repo contains a set of resources and configuration files (.YAML) to generate the dataset offline. Only single resources are provided, there are not full audio scenes. These can be rendered as virtual audio scene using the provided tools.
User can easily add resources (voices, heads, rooms) and define scenes to create new virtual environments and render audio for it.

Dataset configurations are provided as resources too: these are recipes that defines the mix of basic resources to compose the final output. Few examples are already available in this repository.
(t.b.d)

## System Configuration
The code and framework for this repo has been developed and tested in Linux, specifically Ubuntu 20.04. There is no "native" support for Windows environment (t.b.d docker image). You will need a large disk space since even the small datasets can easily generate 100GBbytes due to all the permutations of parameters. The code has been written to leverage multi-processing as much as possible. A moder CPU with at least 8-16 cores is preferred. The code is also tested on AMD Threadripper 3960x/128G and 3990x/256G.

### Installation
After cloning this repository you need to setup your environment (only once) to be able to use VERSE toolchain. The instructions on how to create your virtual environment and fetch resources are available at this page [First Setup](docs/first_setup.md)

## Folder Structure
The structure of the repository is the following:

```
+-- datasets
+-- resources
|   +-- ds_recipes
|   +-- heads
|   +-- paths
|   +-- rooms
|   +-- scenes
|   +-- voices
|
+-- src
|   +-- dataset_render.py
|   +-- scene_render.py
+-- tools
    +-- sound_spatializer
```

DATASETS is the folder that will contain the final audio rendering.

RESOURCES is the folder where all the main components are placed, each one with a repetitive folder structure and [YAML](\url{https://yaml.org/}) decriptors, using the following schema:
```
+-- resources
    +--[RESOURCE_TYPE]
       +--[RESOURCE_NAME]
          +--info.yaml
          +--fetch_files.sh
          +--info
             +--[FILENAME].yaml
          +--files
             +--[AUDIO_FILE].ext
```
RESOURCE_TYPE folder is defined as voices, heads, rooms, paths, scenes. Inside each RESOURCE_TYPE folder there is one sub-folder for each data provider. This allow to pull data of the same type from different locations, even outside of the VERSE repository, as long as the following information are provided. Each RESOURCE_TYPE is defined by a "fetch\_file.sh" script which allows to retrieve binary files from external repositories, and a mandatory "info.yaml" which decribe the resource itself.

Binary files are placed into the "files" subfolder and for each binary file there is a correspondent yaml descriptor into the "info" sub-folder. The purpose of the main info.yaml file is to provide a human readable description of the content for this specific resource, while the purpose of the single yaml files placed into the info sub-folder is to provide a human readable description of each binary files composing the resource itself.

This folder structure is repeated for each resource type, providing a uniform setup to parse available data either manually by human or automatically by code.

# Audio Rendering
Once the environment is setup and all the resources have been prepared you can render audio using two scripts: the "render_scene" and the "render_dataset"

These scripts are the core of the VERSE toolchain: the "render_dataset" script will read a recipe (under "verse/resources/ds_recipes") prepare a set of scene files and launch the "render_scene" multiple times to create audio files in a specific subfolder of "verse/datasets". You can render multiple datasets, they will have separate folders (as long as there is enough space on the disk)

Beside the rendering scripts few other tools are available under "verse/tools/bin":
- display_path
- display_scene
- display_sofa
- parse_sofa
- play_scene

These commands will be explained in the next section (Exploring results / Exploring resources)

## A quick test
To verify if your setup is working correcly you can render the "simple_example" dataset. This is just a testing recipe to create a handful of files using the VERSE toolchain.

```
cd verse/src
/render_dataset.py -i ../resources/ds_recipes/simple_example/info/simple_example.yaml -v
```

Use option "-v" to enable verbose. If your cpu has many cores you can use option "-c" to enable more parallel rendering.

this will create a subfolder with few files under the "verse/datasets" folder :
```
verse/datasets
├── readme.txt
└── simple_example
    └── train
        ├── 000000_static_singlevoice_0_0_0
        │   ├── 000000_static_singlevoice_0_0_0.yaml
        │   ├── static_singlevoice.mkv
        │   └── static_singlevoice_mkv.yaml
        ├── 000001_static_singlevoice_0_0_1
        │   ├── 000001_static_singlevoice_0_0_1.yaml
        │   ├── static_singlevoice.mkv
        │   └── static_singlevoice_mkv.yaml
        ├── 000300_dynamic_singlevoice_1_0_0
        │   ├── 000300_dynamic_singlevoice_1_0_0.yaml
        │   ├── dynamic_singlevoice.mkv
        │   └── dynamic_singlevoice_mkv.yaml
        ├── 000300_dynamic_singlevoice_1_0_1
        │   ├── 000300_dynamic_singlevoice_1_0_1.yaml
        │   ├── dynamic_singlevoice.mkv
        │   └── dynamic_singlevoice_mkv.yaml
        ├── 001300_dynamic_multivoice_0_1_0
        │   ├── 001300_dynamic_multivoice_0_1_0.yaml
        │   ├── dynamic_multivoice.mkv
        │   └── dynamic_multivoice_mkv.yaml
        ├── 001300_dynamic_multivoice_2_0_0
        │   ├── 001300_dynamic_multivoice_2_0_0.yaml
        │   ├── dynamic_multivoice.mkv
        │   └── dynamic_multivoice_mkv.yaml
        ├── 001300_dynamic_multivoice_2_0_1
        │   ├── 001300_dynamic_multivoice_2_0_1.yaml
        │   ├── dynamic_multivoice.mkv
        │   └── dynamic_multivoice_mkv.yaml
        └── 001301_dynamic_multivoice_0_1_1
            ├── 001301_dynamic_multivoice_0_1_1.yaml
            ├── dynamic_multivoice.mkv
            └── dynamic_multivoice_mkv.yaml
```

The .mkv (Matroska)[https://en.wikipedia.org/wiki/Matroska] file will contain the original human voices and the rendered, virtual spatial audio. Two .yaml files are available: one to describe the .mkv content (track by track) and one to describe the audio scene that was used to render the final audio.

All these artifacts can be explored as explained in the section below


## Exploring results
### Play audio
### Display scene


# Resource definition

## Exploring resources

# Dataset definition
