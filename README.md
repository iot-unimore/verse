# VERSE: Virtual Environment for Rendering of Speech Emissions

This is a project focusing on human voice perception with different purposes. 
Among the others, the main goal is to study the advantages of an array of microphones versus binaural audio signals, in the context of real embedded devices and including machine learning algorithms (in particular neural networks) for signal processing, with a particular focus on human voices.

**VERSE** contains a semi-synthetic dataset of voice recordings and real environment characterization measurements.
It includes a complete software framework able to generate synthetic audio data from measures on the real setup, keeping the result acoustically as close as possible to the equivalent "direct recording".

**VERSE** is released as an open source repository serving both functions: providing a flexible dataset and providing a tool to customize the dataset itself based on the specific subject under study.

<img src="/docs/pics/verse_arch_schema.png" align="left" width="200px"/>

Even if VERSE originally was designed to study the performance of an array of microphones placed on a pair of glasses, and to compare results with binaural earing, the flexibility of the framework allows the user to generate sequences using any microphone geometry. This extends the usability of VERSE tool chain to use cases different from multimedia glasses, like robotic heads for industrial applications, or helmets for safety awareness, allowing the optimization of microphone placement to obtain the best results with the provided subject.

**VERSE** is based on the abstraction of main components for an audio scene: voice sound sources from human **speakers**, one **listener** (the "head" of the device under test) and reverberation generated by the environment itself, meaning the **room** hosting speakers and listener. Specific to the definition of a scene is the concept of **motion: the scene defines how sound sources are placed around the listener and how they move in space**.


<br clear="left"/>

# The Dataset
This repo contains a set of resources and configuration files (.YAML) to generate the dataset offline. Only single resources are provided, there are not full audio scenes. These can be rendered as virtual audio scene using the provided tools.
User can easily add resources (voices, heads, rooms) and define scenes to create new virtual environments and render audio for it.

Dataset configurations are provided as resources too: these are recipes that defines the mix of basic resources to compose the final output. Few examples are already available in this repository.
(t.b.d)

## System Configuration
The code and framework for this repo has been developed and tested in Linux, specifically Ubuntu 20.04. There is no "native" support for Windows environment (t.b.d docker image). You will need a large disk space since even the small datasets can easily generate 100GBbytes due to all the permutations of parameters. The code has been written to leverage multi-processing as much as possible. A moder CPU with at least 8-16 cores is preferred. The code is also tested on AMD Threadripper 3960x/128G and 3990x/256G.

### Installation
After cloning this repository you need to setup your environment (only once) to be able to use VERSE toolchain. The instructions on how to create your virtual environment and fetch resources are available at this page [First Setup](docs/first_setup.md)

# Audio Rendering

## A quick test

## Exploring results

## Exploring resources

# Resource definition

# Dataset definition
